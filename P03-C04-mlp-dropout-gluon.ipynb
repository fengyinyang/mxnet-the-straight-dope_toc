{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing DropOut with ``gluon``\n",
    "\n",
    "In [the previous chapter](./P03-C03-mlp-dropout-scratch.ipynb), \n",
    "we introduced DropOut regularization, implementing the algorithm from scratch. \n",
    "As a reminder, DropOut is a regularization technique \n",
    "that zeroes out some fraction of the nodes during training. \n",
    "Then at test time, we use all of the nodes, but scale down their values,\n",
    "essentially averaging the various dropped out nets. \n",
    "If you're approaching the this chapter out of sequence,\n",
    "and aren't sure how DropOut works, it's best to take a look at the implementation by hand\n",
    "since ``gluon`` will manage the low-level details for us.\n",
    "\n",
    "DropOut is a special kind of layer because it behaves differently \n",
    "when training and predicting. \n",
    "We've already seen how ``gluon`` can keep track of when to record vs not record the computation graph.\n",
    "Since this is a ``gluon`` implementation chapter,\n",
    "let's get intro the thick of things by importing our dependencies and some toy data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "ctx = mx.gpu(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()\n",
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "train_data = mx.io.NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"],\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"],\n",
    "                              batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Now we can add DropOut following each of our hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "    \n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\")) \n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the second hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "    \n",
    "    ###########################\n",
    "    # Adding the output layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "\n",
    "Now that we've got an MLP with Dropout, let's register an initializer \n",
    "so we can play with some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mode and predict mode\n",
    "\n",
    "Now that we have an MLP with DropOut, \n",
    "we can grab some data and pass it through the network.\n",
    "We'll actually want to pass the example through the net twice,\n",
    "just to see what effect DropOut is having on our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.21150076  0.30272889  0.17183581 -0.06442011  0.01675556 -0.02324573\n",
      "  -0.04187176 -0.13473594  0.31143227 -0.13674253]]\n",
      "<NDArray 1x10 @gpu(3)>\n",
      "\n",
      "[[ 0.21150076  0.30272889  0.17183581 -0.06442011  0.01675556 -0.02324573\n",
      "  -0.04187176 -0.13473594  0.31143227 -0.13674253]]\n",
      "<NDArray 1x10 @gpu(3)>\n"
     ]
    }
   ],
   "source": [
    "x = train_data.next().data[0].as_in_context(ctx)\n",
    "print(net(x[0]))\n",
    "print(net(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we got the exact same answer on both forward passes through the net!\n",
    "That's because by, default, ``mxnet`` assumes that we are in predict mode. \n",
    "We can explicitly invoke whis scope by placing code within a ``with autograd.predict_mode():`` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.35026801  0.26806137  0.38593394  0.10388274  0.35403782  0.21958774\n",
      "   0.09329571 -0.08700826  0.07742305 -0.29462805]]\n",
      "<NDArray 1x10 @gpu(0)>\n",
      "\n",
      "[[ 0.35026801  0.26806137  0.38593394  0.10388274  0.35403782  0.21958774\n",
      "   0.09329571 -0.08700826  0.07742305 -0.29462805]]\n",
      "<NDArray 1x10 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.predict_mode():\n",
    "    print(net(x[0]))\n",
    "    print(net(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless something's gone horribly wrong, you should see the same result as before. \n",
    "We can also run the code in *train mode*.\n",
    "This tells MXNet to run our Blocks as they would run during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.24313435  0.13018197  0.62095457  0.19018309  0.29342416  0.378932\n",
      "  -0.1570099   0.32591116  0.51068866 -0.23890349]]\n",
      "<NDArray 1x10 @gpu(3)>\n",
      "\n",
      "[[-0.67278039  0.42208618 -0.3246206  -0.04676402  0.26563731  0.3985112\n",
      "  -0.20333797 -0.5953896  -0.01167    -0.2030338 ]]\n",
      "<NDArray 1x10 @gpu(3)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.train_mode():\n",
    "    print(net(x[0]))\n",
    "    print(net(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing ``is_training()`` status\n",
    "\n",
    "You might wonder, how precisely do the Blocks determine \n",
    "whether they should run in train mode or predict mode?\n",
    "Basically, autograd maintains a Boolean state \n",
    "that can be accessed via ``autograd.is_training()``. \n",
    "By default this falue is ``False`` in the global scope.\n",
    "This way if someone just wants to make predictions and \n",
    "doesn't know anything about training models, everything will just work.\n",
    "When we enter a ``train_mode()`` block, \n",
    "we create a scope in which ``is_training()`` returns ``True``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with autograd.predict_mode():\n",
    "    print(autograd.is_training())\n",
    "    \n",
    "with autograd.train_mode():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with ``autograd.record``\n",
    "\n",
    "When we train, neural network models,\n",
    "we nearly always enter ``record()`` blocks.\n",
    "The purpose of ``record()`` is to build the computational graph.\n",
    "And the purpose of ``train`` is to indicate that we are training our model.\n",
    "These two are highly correlated but should not be confused.\n",
    "For example, when we generate adversarial examples (a topic we'll investigate later)\n",
    "we may want to record, but for the model to behave as in predict mode.\n",
    "On the other hand, sometimes, even when we're not recording,\n",
    "we still want to evaluate the model's training behavior.\n",
    "\n",
    "A problem then arises. Since ``record()`` and ``train_mode()``\n",
    "are distinct, how do we avoid having to declare two scopes every time we train the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "#  Writing this every time could get cumbersome\n",
    "##########################\n",
    "with autograd.record():\n",
    "    with autograd.train_mode():\n",
    "        yhat = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our lives a little easier, record() takes one argument, ``train_mode``,\n",
    "which has a default value of True.\n",
    "So when we turn on autograd, this by default turns on train_mode\n",
    "(``with autograd.record()`` is equivalent to\n",
    "``with autograd.record(train_mode=True):``).\n",
    "To change this default behavior\n",
    "(as when generating adversarial examples),\n",
    "we can optionally call record via\n",
    "(``with autograd.record(train_mode=False):``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx).reshape((-1, 784))\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.329100704774, Train_acc 0.937849813433, Test_acc 0.937300955414\n",
      "Epoch 1. Loss: 0.248198956118, Train_acc 0.959005197228, Test_acc 0.958499203822\n",
      "Epoch 2. Loss: 0.200681836518, Train_acc 0.967667244136, Test_acc 0.962977707006\n",
      "Epoch 3. Loss: 0.179167188224, Train_acc 0.972098214286, Test_acc 0.968451433121\n",
      "Epoch 4. Loss: 0.162256601359, Train_acc 0.975679637527, Test_acc 0.969844745223\n",
      "Epoch 5. Loss: 0.149811580049, Train_acc 0.978294909382, Test_acc 0.972332802548\n",
      "Epoch 6. Loss: 0.128341759589, Train_acc 0.981343283582, Test_acc 0.973128980892\n",
      "Epoch 7. Loss: 0.126888067755, Train_acc 0.983142324094, Test_acc 0.976015127389\n",
      "Epoch 8. Loss: 0.125234942328, Train_acc 0.985824227079, Test_acc 0.975915605096\n",
      "Epoch 9. Loss: 0.112150726553, Train_acc 0.985157915778, Test_acc 0.977010350318\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx).reshape((-1, 784))\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now let's take a look at how to build convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
